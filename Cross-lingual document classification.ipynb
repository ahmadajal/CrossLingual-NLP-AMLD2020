{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "Cross-lingual document classification.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx12xHQsBEY7",
        "colab_type": "text"
      },
      "source": [
        "# Problem description\n",
        "\n",
        "Cross-lingual document classification (CLDC) is the text mining problem where we are given:\n",
        "- labeled documents for training in a source language $\\ell_1$, and \n",
        "- test documents written in a target language $\\ell_2$. \n",
        "\n",
        "For example, the training documents are written in English, and the test documents are written in French. \n",
        "\n",
        "\n",
        "CLDC is an interesting problem. The hope is that we can use resource-rich languages to train models that can be applied to resource-deprived languages. This would result in transferring knowledge from one language to another. \n",
        "There are several methods that can be used in this context. In this workshop we start from naive approaches and progressively introduce more complex solutions. \n",
        "\n",
        "The most naive solution is to ignore the fact the training and test documents are written in different languages.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh_ZYfs3BEY9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e25fb057-f346-4c09-bc66-43f04ff3182d"
      },
      "source": [
        "import sys, os\n",
        "!git clone https://github.com/ioannispartalas/CrossLingual-NLP-AMLD2020.git\n",
        "sys.path.append('CrossLingual-NLP-AMLD2020')\n",
        "\n",
        "import pandas as pd\n",
        "from ast import literal_eval\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils.validation import check_X_y\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from collections import Counter\n",
        "from utils import *"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CrossLingual-NLP-AMLD2020'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 14 (delta 3), reused 10 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (14/14), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkCDFdQCBEZA",
        "colab_type": "text"
      },
      "source": [
        "1. Dataset: holds the data of sources and target language\n",
        "2. System: This is a set of steps: Does fit, predict. Can be in the form of a pipeline also\n",
        "3. Experiment: Given a Dataset and a System it fits, predicts and reports evaluation scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmJAHXeTBEZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset:\n",
        "    \"\"\"Experiment class, that reads data in raw format and prints stats.\"\"\"\n",
        "    def __init__(self, pathtodata,source_lang, target_lang):\n",
        "        self.source_lang = source_lang\n",
        "        self.target_lang = target_lang\n",
        "        self.tr_path = pathtodata+\"semeval15.%s.train.csv\" % source_lang\n",
        "        self.te_path = pathtodata+\"semeval15.%s.test.csv\" % target_lang\n",
        "    \n",
        "    @staticmethod\n",
        "    def read_csv(path):\n",
        "        df = pd.read_csv(path)\n",
        "        df['polarities'] = df['polarities'].apply(lambda l: literal_eval(l))\n",
        "        df = df.loc[df.polarities.astype(bool)]\n",
        "        df['sentiment'] = df['polarities'].apply(lambda l: Counter(l).most_common(1)[0][0])\n",
        "        return df[['text', 'sentiment']]\n",
        "\n",
        "    def load_data(self):\n",
        "        training = self.read_csv(self.tr_path)\n",
        "        test = self.read_csv(self.te_path)\n",
        "        print(\"\\nTraining data\\n==========\")\n",
        "        self.calculate_stats(training)\n",
        "        print(\"\\nTraining data\\n==========\")\n",
        "        self.calculate_stats(test)\n",
        "        self.train, self.y_train = training.text.values, training.sentiment.values\n",
        "        self.test, self.y_test = test.text.values, test.sentiment.values\n",
        "\n",
        "    # Function to load the Cross-lingual embeddings for each language\n",
        "    def load_cl_embeddings(self,path_to_embeddings,dimension,skip_header):\n",
        "        self.vocab_source = fit_vocab(self.train)\n",
        "        self.vocab_target = fit_vocab(self.test)\n",
        "        \n",
        "        # full vocabulary\n",
        "        self.vocab_ = fit_vocab(np.concatenate((self.train,self.test)))\n",
        "        \n",
        "        self.source_embeddings = load_embeddings(path_to_embeddings+\"concept_net_1706.300.\"+self.source_lang, dimension,skip_header=skip_header,vocab=self.vocab_)\n",
        "        self.target_embeddings = load_embeddings(path_to_embeddings+\"concept_net_1706.300.\"+self.target_lang, dimension,skip_header=skip_header,vocab=self.vocab_)\n",
        "        \n",
        "        self.source_embeddings = sort_embeddings(self.source_embeddings,self.vocab_)\n",
        "        self.target_embeddings = sort_embeddings(self.target_embeddings,self.vocab_)\n",
        "        \n",
        "        \n",
        "    def calculate_stats(self, df):\n",
        "        print(\"Training Data Shape: \", df.shape)\n",
        "        print(\"Class distribution: \", df.sentiment.value_counts().to_dict())\n",
        "\n",
        "        \n",
        "class Runner:\n",
        "    def __init__(self, pipeline, experiment):\n",
        "        self.pipeline = pipeline\n",
        "        self.experiment = experiment\n",
        "        #self.experiment.load_data()\n",
        "        \n",
        "    def score(self, preds):\n",
        "        #return accuracy_score(exp.y_test, preds)\n",
        "        return f1_score(exp.y_test, preds,average=\"macro\")\n",
        "    def eval_system(self):\n",
        "        pipeline.fit(exp.train, exp.y_train)\n",
        "        preds = pipeline.predict(exp.test)\n",
        "        scores = self.score(preds)\n",
        "        return scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjB3tOXPBEZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class nBowClassifier(BaseEstimator, ClassifierMixin):\n",
        "\n",
        "    def __init__(self, base_classifier = \"knn\",V_source=None,V_target=None,params={}):\n",
        "        self.base_classifier = base_classifier\n",
        "        self.V_source = V_source\n",
        "        self.V_target = V_target\n",
        "        self.params = params\n",
        "        \n",
        "        if base_classifier ==\"knn\":\n",
        "            self.clf = KNeighborsClassifier(**params)\n",
        "        #elif base_classifier==\"mlp\" #TODO: add support for an MPL classifier\n",
        "        else:\n",
        "            raise ValueError(\"Unknown base classifier\")\n",
        "\n",
        "    # neural bag-of-words baseline\n",
        "    # average word embeddings of each document\n",
        "    # V_emb: this holds the embedding of each word\n",
        "    # X: the vectorized array of documents. Note that the indices of the features should correspond to the same indices in the V_emb array\n",
        "    def _nBOW(self,V_emb,X):\n",
        "        X_avg = []\n",
        "        for doc in X:\n",
        "            doc_vecs = V_emb[doc.indices,:]\n",
        "            avg_vec = np.sum((doc_vecs*doc.data[:,np.newaxis]),axis=0)/(doc.data.sum() + 1.0)\n",
        "            X_avg.append(avg_vec)\n",
        "        \n",
        "        return np.array(X_avg)\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "\n",
        "        # Store the classes seen during fit\n",
        "        self.classes_ = unique_labels(y)\n",
        "        \n",
        "        X_avg = self._nBOW(self.V_source,X)\n",
        "        self.n_samples, self.n_features = X_avg.shape\n",
        "        \n",
        "        # Check that X and y have correct shape\n",
        "        X_avg, y = check_X_y(X_avg, y,accept_sparse=False)\n",
        "        \n",
        "        self.clf.fit(X_avg,y)\n",
        "        # Return the classifier\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        # Check is fit had been called\n",
        "        # check_is_fitted(self, ['X_', 'y_'])\n",
        "\n",
        "        # Input validation\n",
        "        #X = check_array(X)\n",
        "        X_avg = self._nBOW(self.V_target,X)\n",
        "        predictions = self.clf.predict(X_avg)\n",
        "        \n",
        "        return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUmq1lqmBEZF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "ddbcc87d-583f-4315-c972-eb82da14c7c5"
      },
      "source": [
        "exp = Dataset(\"../NLP/sentiment_classification/data/clean_data/\",\"en\", \"es\")\n",
        "exp.load_data()\n",
        "exp.load_cl_embeddings(\"../NLP/sentiment_classification/embeddings/\",300,False)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d912b203a73c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../NLP/sentiment_classification/data/clean_data/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"es\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_cl_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../NLP/sentiment_classification/embeddings/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2d47f6829f11>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mte_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTraining data\\n==========\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2d47f6829f11>\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'polarities'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'polarities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../NLP/sentiment_classification/data/clean_data/semeval15.en.train.csv' does not exist: b'../NLP/sentiment_classification/data/clean_data/semeval15.en.train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rEZNi1ABEZI",
        "colab_type": "code",
        "colab": {},
        "outputId": "2c3476b2-40d0-437b-84e9-33f4ad1d5e59"
      },
      "source": [
        "# Majority Class\n",
        "pipeline = Pipeline([('vectorizer', CountVectorizer()), \n",
        "                     ('classifier', DummyClassifier())])\n",
        "runner = Runner(pipeline, exp)\n",
        "runner.eval_system()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3278293686791141"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwDcvOufBEZL",
        "colab_type": "code",
        "colab": {},
        "outputId": "aac9dc41-77e6-41d0-c085-572044759f8d"
      },
      "source": [
        "# Logistic Regression on words\n",
        "pipeline = Pipeline([('vectorizer', CountVectorizer(lowercase=True)), \n",
        "                     ('classifier', LogisticRegression(solver=\"lbfgs\"))])\n",
        "runner = Runner(pipeline, exp)\n",
        "runner.eval_system()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/ipartalas/virtual_envs/semeval-exps/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/home/ipartalas/virtual_envs/semeval-exps/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.38649930974693864"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZXLpJ8HBEZM",
        "colab_type": "code",
        "colab": {},
        "outputId": "735021f3-70b3-410e-d740-a659eafe679a"
      },
      "source": [
        "params = {\"n_neighbors\":5}\n",
        "avg_baseline = nBowClassifier(\"knn\",exp.source_embeddings,exp.target_embeddings,params)\n",
        "\n",
        "pipeline = Pipeline([('vectorizer', CountVectorizer(lowercase=True,vocabulary=exp.vocab_)), \n",
        "                     ('classifier', avg_baseline)])\n",
        "\n",
        "runner = Runner(pipeline, exp)\n",
        "runner.eval_system()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.49088213651906676"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shYVJ3aPBEZO",
        "colab_type": "code",
        "colab": {},
        "outputId": "7cbd4677-4df3-41c4-dcec-b5789327f247"
      },
      "source": [
        "from models import *"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/ipartalas/projects/LASER/\n",
            "/home/ipartalas/projects/LASER/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agX3pNXwBEZR",
        "colab_type": "code",
        "colab": {},
        "outputId": "4f880941-6d33-402e-8269-4647be11e107"
      },
      "source": [
        "params = {\"n_neighbors\":5}\n",
        "avg_baseline = LASERClassifier(\"knn\",exp.source_lang,exp.target_lang,params)\n",
        "\n",
        "pipeline = Pipeline([('classifier', avg_baseline)])\n",
        "\n",
        "runner = Runner(pipeline, exp)\n",
        "runner.eval_system()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " - Encoder: loading /home/ipartalas/projects/LASER/models/bilstm.93langs.2018-12-26.pt\n",
            " - Tokenizer: tmpzo3b2kaz in language en  \n",
            " - fast BPE: processing tok\n",
            " - Encoder: bpe to out.raw\n",
            " - Encoder: 1708 sentences in 8s\n",
            " - Encoder: loading /home/ipartalas/projects/LASER/models/bilstm.93langs.2018-12-26.pt\n",
            " - Tokenizer: tmpuaukb8yg in language es  \n",
            " - fast BPE: processing tok\n",
            " - Encoder: bpe to out.raw\n",
            " - Encoder: 677 sentences in 8s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5207454999203526"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3eS4mHsBEZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}